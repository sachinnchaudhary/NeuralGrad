# NeuralGrad
Inspired by Andrej Karpathy's implementation of micrograd, this is the similar kind of library which can perform operations related to neural networks like fforward pass and back propagation, loss computation and perfoming gradient descent with minimal code(near 150-200 lines probably). i want to implement even more things so additionally i added importat activation functions like Relu, Sigmoid and loss function such as Cross entropy and MSE(mean squere error) with Optimization algorithm(RMSprop and Adam). The idea behind this is to get intution around how the process happens behind the hood in deep learning frameworks like Pytorch. 


